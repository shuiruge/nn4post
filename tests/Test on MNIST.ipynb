{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuiruge/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# -- `contrib` module in TF 1.3\n",
    "from tensorflow.contrib.distributions import (\n",
    "    NormalWithSoftplusScale, Categorical,\n",
    ")\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from nn4post import Inferencer\n",
    "from nn4post.utils import get_param_shape, get_param_space_dim\n",
    "from nn4post.utils.posterior import get_log_posterior\n",
    "from nn4post.utils.tf_trainer import SimpleTrainer\n",
    "\n",
    "import mnist\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "N_C = 1\n",
    "NOISE_STD = 0.0\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "# DATA\n",
    "mnist_ = mnist.MNIST(NOISE_STD, BATCH_SIZE)\n",
    "\n",
    "\n",
    "# MODEL\n",
    "n_inputs = 28 * 28  # number of input features.\n",
    "n_hiddens = 200  # number of perceptrons in the (single) hidden layer.\n",
    "n_outputs = 10  # number of perceptrons in the output layer.\n",
    "\n",
    "with tf.name_scope('data'):\n",
    "    x = tf.placeholder(shape=[None, n_inputs], dtype=tf.float32, name='x')\n",
    "    y = tf.placeholder(shape=[None], dtype=tf.int32, name='y')\n",
    "\n",
    "input_ = {'x': x}\n",
    "observed = {'y': y}\n",
    "\n",
    "def model(input_, param):\n",
    "    \"\"\" Shall be implemented by TensorFlow. This is an example, as a shallow\n",
    "    neural network.\n",
    "\n",
    "    Args:\n",
    "        input_:\n",
    "            `dict`, like `{'x_1': x_1, 'x_2': x_2}, with values Tensors.\n",
    "        param:\n",
    "            `dict`, like `{'w': w, 'b': b}, with values Tensors.\n",
    "\n",
    "    Returns:\n",
    "        `dict`, like `{'y': Y}`, where `Y` is an instance of\n",
    "        `tf.distributions.Distribution`.\n",
    "    \"\"\"\n",
    "    # shape: `[None, n_hiddens]`\n",
    "    hidden = tf.sigmoid(\n",
    "        tf.matmul(input_['x'], param['w_h']) + param['b_h'])\n",
    "    # shape: `[None, n_outputs]`\n",
    "    logits = tf.matmul(hidden, param['w_a']) + param['b_a']\n",
    "\n",
    "    Y = Categorical(logits=logits)\n",
    "    return {'y': Y}\n",
    "\n",
    "\n",
    "# PRIOR\n",
    "with tf.name_scope('prior'):\n",
    "    w_h = NormalWithSoftplusScale(\n",
    "        loc=tf.zeros([n_inputs, n_hiddens]),\n",
    "        scale=tf.ones([n_inputs, n_hiddens]) * 10,\n",
    "        name=\"w_h\")\n",
    "    w_a = NormalWithSoftplusScale(\n",
    "        loc=tf.zeros([n_hiddens, n_outputs]),\n",
    "        scale=tf.ones([n_hiddens, n_outputs]) * 10,\n",
    "        name=\"w_a\")\n",
    "    b_h = NormalWithSoftplusScale(\n",
    "        loc=tf.zeros([n_hiddens]),\n",
    "        scale=tf.ones([n_hiddens]) * 100,\n",
    "        name=\"b_h\")\n",
    "    b_a = NormalWithSoftplusScale(\n",
    "        loc=tf.zeros([n_outputs]),\n",
    "        scale=tf.ones([n_outputs]) * 100,\n",
    "        name=\"b_a\")\n",
    "\n",
    "param_prior = {\n",
    "    'w_h': w_h, 'w_a': w_a,\n",
    "    'b_h': b_h, 'b_a': b_a,\n",
    "}\n",
    "\n",
    "\n",
    "# POSTERIOR\n",
    "scale = mnist_.n_data / mnist_.batch_size\n",
    "log_posterior = get_log_posterior(\n",
    "    model, input_, observed, param_prior, scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-- Dimension of parameter-space: 159010.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# INFERENCE\n",
    "param_shape = get_param_shape(param_prior)\n",
    "param_space_dim = get_param_space_dim(param_shape)\n",
    "print('\\n-- Dimension of parameter-space: {}.\\n'.format(param_space_dim))\n",
    "\n",
    "inferencer = Inferencer(N_C, param_space_dim, log_posterior)\n",
    "\n",
    "with tf.name_scope('variables'):\n",
    "    a = tf.Variable(\n",
    "        np.zeros([N_C]),\n",
    "        dtype='float32',\n",
    "        name='a')\n",
    "    mu = tf.Variable(\n",
    "        np.random.normal(size=[N_C, param_space_dim]),\n",
    "        dtype='float32',\n",
    "        name='mu')\n",
    "    zeta = tf.Variable(\n",
    "        np.zeros([N_C, param_space_dim]),\n",
    "        dtype='float32',\n",
    "        name='zeta')\n",
    "\n",
    "var = {'a': a, 'mu': mu, 'zeta': zeta}\n",
    "loss, gradients = inferencer.make_loss_and_gradients(**var)\n",
    "samples, weights = inferencer.make_samples_and_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Summary name nn4post/loss/loss/add:0 is illegal; using nn4post/loss/loss/add_0 instead.\n",
      "INFO:tensorflow:Summary name nn4post/loss/loss/add:0 is illegal; using nn4post/loss/loss/add_0 instead.\n",
      "INFO:tensorflow:Restoring parameters from ../dat/checkpoints/nn4post_advi_on_mnist/checkpoint-120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Restored from ../dat/checkpoints/nn4post_advi_on_mnist/.\n",
      "INFO - Start training at global step 120.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN\n",
    "batch_generator = mnist_.batch_generator()\n",
    "def get_feed_dict_generator():\n",
    "    while True:\n",
    "        x_train, y_train, y_err_train = next(batch_generator)\n",
    "        y_train = np.argmax(y_train, axis=1).astype('int32')\n",
    "        yield {x: x_train, y: y_train}\n",
    "trainer = SimpleTrainer(\n",
    "    loss=loss,\n",
    "    gvs=gradients,\n",
    "    optimizer=tf.train.AdamOptimizer(0.005),\n",
    "    logdir='../dat/logs/nn4post_advi_on_mnist',\n",
    "    dir_to_ckpt='../dat/checkpoints/nn4post_advi_on_mnist/',\n",
    ")\n",
    "#n_iters = 30000\n",
    "n_iters = 0  # test!\n",
    "feed_dict_generator = get_feed_dict_generator()\n",
    "trainer.train(n_iters, feed_dict_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# SAMPLING\n",
    "a_val, mu_val, zeta_val = trainer.sess.run([a, mu, zeta])\n",
    "x_val, y_val, *rests = next(batch_generator)\n",
    "y_val = np.argmax(y_val, axis=1).astype('int32')\n",
    "feed_dict = {\n",
    "    inferencer.q_parameters.a: a_val,\n",
    "    inferencer.q_parameters.mu: mu_val,\n",
    "    inferencer.q_parameters.zeta: zeta_val,\n",
    "    inferencer.n_pred_samples: 10,\n",
    "    x: x_val,\n",
    "    y: y_val,\n",
    "}\n",
    "sample_vals, weight_vals = trainer.sess.run(\n",
    "    [samples, weights],\n",
    "    feed_dict=feed_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 159010), (10,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_vals.shape, weight_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION\n",
    "\n",
    "# Get test data of MNIST\n",
    "x_test, y_test, y_err_test = mnist_.test_data\n",
    "# Adjust to the eagered form\n",
    "y_test = y_test.astype('int32')\n",
    "\n",
    "\n",
    "# Get the trained variables.\n",
    "trained_var = {\n",
    "    name:\n",
    "        trainer.sess.run(v)\n",
    "    for name, v in var.items()\n",
    "}\n",
    "print('a: ', trained_var['a'])\n",
    "print('zeta mean: ', np.mean(trained_var['zeta']))\n",
    "print('zeta std: ', np.std(trained_var['zeta']))\n",
    "\n",
    "predictions_dict = build_prediction(\n",
    "    trained_var, model, param_shape, input_, n_samples=100)\n",
    "predictions = tf.stack(predictions_dict['y'], axis=0)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    feed_dict = {x: x_test}\n",
    "    # shape: `[n_samples, n_data]`\n",
    "    predictions = sess.run(predictions, feed_dict=feed_dict)\n",
    "\n",
    "def get_most_freq(array):\n",
    "    index = np.argmax(np.bincount(array))\n",
    "    return index\n",
    "# shape `[n_data]`\n",
    "voted_predictions = np.array([\n",
    "    get_most_freq(predictions[:,i])\n",
    "    for i in range(predictions.shape[1])\n",
    "])\n",
    "\n",
    "def get_accuracy(xs, ys):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        xs:\n",
    "            Numpy array.\n",
    "        ys:\n",
    "            Numpy array with the same shape and dtype as `xs`.\n",
    "\n",
    "    Returns:\n",
    "        `float` as the perception of `x == y` in `xs`, where `x` and `y` in\n",
    "        `xs` and `ys` respectively and one-to-one correspondently.\n",
    "    \"\"\"\n",
    "    assert xs.dtype == ys.dtype\n",
    "    n_correct = 0\n",
    "    n_mistake = 0\n",
    "    for x, y in list(zip(xs, ys)):\n",
    "        if x == y:\n",
    "            n_correct += 1\n",
    "        else:\n",
    "            n_mistake += 1\n",
    "    return n_correct / (n_correct + n_mistake)\n",
    "\n",
    "targets = y_test  # shape `[n_data]`, dtype `int32`.\n",
    "voted_predictions = voted_predictions.astype('int32')\n",
    "accuracy = get_accuracy(voted_predictions, targets)\n",
    "print('Accuracy: ', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
