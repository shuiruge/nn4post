{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Trial on Gaussian Mixture Distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shuiruge/anaconda/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.distributions import (\n",
    "    Categorical, NormalWithSoftplusScale, Mixture)\n",
    "\n",
    "from nn4post import InferenceBuilder\n",
    "try:\n",
    "    from tensorflow.contrib.distributions import Independent\n",
    "except:\n",
    "    print('WARNING - Your TF < 1.4.0.')\n",
    "    from nn4post.utils.independent import Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "SEED = 123456\n",
    "tf.set_random_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_log_posterior(target_c, target_mu, target_zeta):\n",
    "  \n",
    "    target_c = tf.convert_to_tensor(target_c)\n",
    "    target_mu = tf.convert_to_tensor(target_mu)\n",
    "    target_zeta = tf.convert_to_tensor(target_zeta)\n",
    "\n",
    "    # -- Gaussian Mixture Distribution\n",
    "    with tf.name_scope('posterior'):\n",
    "\n",
    "      cat = Categorical(probs=target_c)\n",
    "      components = [\n",
    "          Independent(\n",
    "              NormalWithSoftplusScale(target_mu[i], target_zeta[i])\n",
    "          ) for i in range(target_c.shape[0])\n",
    "      ]\n",
    "      p = Mixture(cat, components)\n",
    "\n",
    "      def log_posterior(theta):\n",
    "          return p.log_prob(theta)\n",
    "\n",
    "    return log_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def shall_stop(loss_values, tolerance, n_means=20):\n",
    "    \"\"\"Returns `True` if the relative variance of loss-value in\n",
    "    `loss_values` becomes smaller than `tolerance`, else `False`.\n",
    "    The loss-value is smeared by its nearby `n_means` loss-values.\n",
    "    \"\"\"\n",
    "    if len(loss_values) < 2 * n_means:\n",
    "        return False\n",
    "\n",
    "    else:\n",
    "        previous_loss = np.mean(loss_values[-2*n_means:-n_means])\n",
    "        current_loss = np.mean(loss_values[-n_means:])\n",
    "        delta_loss = previous_loss - current_loss\n",
    "        relative_delta_loss = abs( delta_loss / (current_loss + 1e-8) )\n",
    "\n",
    "        if relative_delta_loss < tolerance:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "def test(target_c, target_mu, target_zeta, init_var,\n",
    "         tolerance=1e-2, n_iters=None, **inference_kwargs):\n",
    "    \"\"\"Test on Gaussian mixture distribution as the target.\n",
    "    \n",
    "    Args:\n",
    "        target_c: Numpy array.\n",
    "        target_mu: Numpy array.\n",
    "        target_zeta: Numpy array.\n",
    "        init_var: Dictionary with keys: \"a\", \"mu\", and \"zeta\" and\n",
    "            values numpy arraies.\n",
    "        XXX\n",
    "            `n_iters` is more preferable than `tolerance`.\n",
    "        inference_kwargs: Dictionary, as the kwargs (parameters)\n",
    "            of `InferenceBuilder.__init__()`.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with keys: \"loss\", \"a\", \"mu\", and \"zeta\", and\n",
    "        values the values at each iteration, collected in a list.\n",
    "    \"\"\"\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    log_p = make_log_posterior(target_c, target_mu, target_zeta)\n",
    "\n",
    "    n_c, n_d = init_var['mu'].shape\n",
    "    ib = InferenceBuilder(n_c, n_d, log_p, **inference_kwargs)\n",
    "    \n",
    "    a = tf.Variable(init_var['a'], dtype='float32')\n",
    "    mu = tf.Variable(init_var['mu'], dtype='float32')\n",
    "    zeta = tf.Variable(init_var['zeta'], dtype='float32')\n",
    "    loss, gradients = ib.make_loss_and_gradients(a, mu, zeta)\n",
    "\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.05)\n",
    "    #optimizer = tf.train.AdamOptimizer(0.005)\n",
    "    train_op = optimizer.apply_gradients(gradients)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        sess.run(init)\n",
    "\n",
    "        test_result = {'loss': [], 'a': [], 'mu': [], 'zeta': []}\n",
    "        step = 0\n",
    "            \n",
    "        def iter_body():\n",
    "            \n",
    "            nonlocal step\n",
    "\n",
    "            _, loss_val, a_val, mu_val, zeta_val \\\n",
    "                = sess.run([train_op, loss, a, mu, zeta])\n",
    "\n",
    "            test_result['loss'].append(loss_val)\n",
    "            test_result['a'].append(a_val)\n",
    "            test_result['mu'].append(mu_val)\n",
    "            test_result['zeta'].append(zeta_val)\n",
    "            \n",
    "            step += 1\n",
    "            if (step+1) % 100 == 0:\n",
    "                print(step, loss_val)\n",
    "\n",
    "        if n_iters:\n",
    "            for i in range(n_iters):\n",
    "                iter_body()\n",
    "                \n",
    "        else:\n",
    "            while not shall_stop(test_result['loss'], tolerance):\n",
    "                iter_body()\n",
    "                \n",
    "    return test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helpers\n",
    "\n",
    "def softplus(x, limit=10.):\n",
    "    return np.where(x<limit, np.log(1. + np.exp(x)), x)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "\n",
    "def plot_trajectory(trajectory, i=0, j=1):\n",
    "    \"\"\"Plot the 2D projection of the trajectory `trajectory`.\n",
    "    \n",
    "    Args:\n",
    "        trajectory: List of 2D numpy array.\n",
    "        i: Integer, as the x-axis to be plotted, optional.\n",
    "        j: Integer, as the y-axis to be plotted, optional.\n",
    "    \"\"\"\n",
    "    n_points = len(trajectory)\n",
    "    x = [_[i] for _ in trajectory]\n",
    "    y = [_[j] for _ in trajectory]\n",
    "    delta_x = [x[i+1] - x[i] for i in range(n_points-1)]\n",
    "    delta_y = [y[i+1] - y[i] for i in range(n_points-1)]\n",
    "    plt.quiver(x, y, delta_x, delta_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Effect of $\\beta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_d = 1000\n",
    "ones = np.ones([n_d]).astype('float32')\n",
    "target_c = np.array([0.7, 0.25, 0.05]).astype('float32')\n",
    "target_mu = np.array([-2*ones, 0*ones, 2*ones]).astype('float32')\n",
    "target_zeta = np.array([ones, ones, ones]).astype('float32')\n",
    "\n",
    "n_c = 10\n",
    "init_var = {\n",
    "    'a': np.zeros([n_c]).astype('float32'),\n",
    "    'mu': np.random.normal(0., 30., size=[n_c, n_d]).astype('float32'),\n",
    "    'zeta': np.zeros([n_c, n_d]).astype('float32'),\n",
    "}\n",
    "\n",
    "init_var['mu'][0] = -2 * np.ones([n_d]).astype('float32')\n",
    "\n",
    "tolerance = 1e-3\n",
    "n_iters = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When $\\beta = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 1265.1896\n",
      "199 120.56723\n",
      "299 91.90917\n",
      "399 69.67917\n",
      "499 52.085434\n",
      "599 37.83327\n",
      "699 26.693905\n",
      "799 18.820646\n",
      "899 13.792197\n",
      "999 9.87311\n",
      "1099 7.0486827\n",
      "1199 5.578548\n",
      "1299 4.45383\n",
      "1399 3.5842123\n",
      "1499 3.2381592\n",
      "1599 2.571051\n",
      "1699 2.4567008\n",
      "1799 2.3262167\n",
      "1899 2.1289198\n",
      "1999 1.9581183\n",
      "CPU times: user 2h 26min 59s, sys: 18min 55s, total: 2h 45min 54s\n",
      "Wall time: 1d 9h 22min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "test_result_0 = test(target_c, target_mu, target_zeta,init_var,\n",
    "                     tolerance=tolerance, n_iters=n_iters, beta=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* When $\\beta = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 483.03787\n",
      "199 93.27321\n",
      "299 68.1297\n",
      "399 49.337177\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# The same configuration as `test_result_0`, but with `beta=1.0`\n",
    "test_result_1 = test(target_c, target_mu, target_zeta,init_var,\n",
    "                     tolerance=tolerance, n_iters=n_iters, beta=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def display(test_result):\n",
    "    \n",
    "    print('Final loss:', np.mean(test_result['loss'][-20:]))\n",
    "    \n",
    "    for i in range(n_c):\n",
    "\n",
    "        trajectory = [_[i,:] for _ in test_result['mu']]\n",
    "        plot_trajectory(trajectory)\n",
    "        plt.show()\n",
    "\n",
    "        c = [softmax(_) for _ in test_result['a']]\n",
    "        plt.plot([_[i] for _ in c])\n",
    "        plt.show()\n",
    "\n",
    "        print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(test_result_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display(test_result_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [i for i in range(n_iters)]\n",
    "plt.plot(x, test_result_0['loss'], color='green')\n",
    "plt.plot(x, test_result_1['loss'], color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The loss of $\\beta = 1$ converges manifestly faster than that of $\\beta = 0$.\n",
    "* The final effect of $\\beta = 1$ looks better than $\\beta = 0$. But the improvement is little."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad Initial Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_d = 2\n",
    "ones = np.ones([n_d]).astype('float32')\n",
    "target_c = np.array([0.8, 0.2]).astype('float32')\n",
    "target_mu = np.array([-2*ones, 2*ones]).astype('float32')\n",
    "target_zeta = np.array([ones, ones, ones]).astype('float32')\n",
    "\n",
    "n_c = 10\n",
    "init_var = {\n",
    "    'a': np.zeros([n_c]).astype('float32'),\n",
    "    'mu': np.random.normal(0., 5., size=[n_c, n_d]).astype('float32'),\n",
    "    'zeta': np.zeros([n_c, n_d]).astype('float32'),\n",
    "}\n",
    "\n",
    "init_var['mu'][0] = -2 * np.ones([n_d]).astype('float32')\n",
    "init_var['mu'][1] = 2 * np.ones([n_d]).astype('float32')\n",
    "\n",
    "tolerance = 1e-3\n",
    "n_iters = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test_result = test(target_c, target_mu, target_zeta,init_var,\n",
    "                   tolerance=tolerance, n_iters=n_iters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
