\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,bbm,hyperref}

%%%%%%%%%% Start TeXmacs macros
\catcode`\>=\active \def>{
\fontencoding{T1}\selectfont\symbol{62}\fontencoding{\encodingdefault}}
\catcode`\|=\active \def|{
\fontencoding{T1}\selectfont\symbol{124}\fontencoding{\encodingdefault}}
\newcommand{\assign}{:=}
\newcommand{\mathd}{\mathrm{d}}
\newcommand{\tmem}[1]{{\em #1\/}}
\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
%%%%%%%%%% End TeXmacs macros

\begin{document}

\section{Notations}

Let $f ( x; \theta )$ a function of $x$ with parameter $\theta$. Let $y=f ( x;
\theta )$ an observable, thus the observed value obeys a Gaussian
distribution. Let $D$ denotes a set of observations, $D \assign \{ ( x_{i}
,y_{i} , \sigma_{i} ) :i=1, \ldots ,N_{D} \}$, wherein $x_{i}$ is the $i$th
input, $y_{i}$ its observed value, and $\sigma_{i}$ the observational error of
$y_{i}$. We may employ mini-batch technique, thus denote $D_{m} \assign \{ (
x_{i} ,y_{i} , \sigma_{i} ) :i=1, \ldots ,N_{m} \} \subset D$ as a mini-batch,
with batch-size $N_{m} \leqslant N_{D}$. We use $\mathbbm{E}_{f ( \theta )} [
g ( \theta ) ]$ represent the expectation of function $g$ of a random variable
obeys the p.d.f. $f$. $\Phi$ is for Gaussian p.d.f..

\section{Neural Network for Posterior (nn4post)}

\subsection{The Model}

Suppose we have some prior on $\theta$, $p ( \theta )$, we gain the
un-normalized posterior $p ( D| \theta )  p ( \theta )$. With $D$ arbitrarily
given, this un-normalized posterior is a function of $\theta$, denoted by $p (
\theta ;D )$\footnote{This is why we use `` $;$ '' instead of `` $,$ '',
indicating that $D$ has been (arbitrarily) given and fixed.}.\footnote{The
normalized posterior $p ( \theta |D ) =p ( D| \theta )  p ( \theta ) /p ( D )
=p ( \theta ;D ) /p ( D )$, by Bayes's rule.}

We we are going to do is fit this $p ( \theta ;D )$ by ANN for any given $D$.
To do so, we have to assume that $\tmop{supp} \{ p ( \theta ;D ) \}
=\mathbbm{R}^{d}$ for some $d \in \mathbbm{N}^{+}$ (i.e. has no compact
support) but decrease exponentially fast as $\| \theta \| \rightarrow +
\infty$. With this assumption, $\ln  p ( \theta ;D )$ is well-defined. For
ANN, we propose using Gaussian function as the activation-function. Thus, we
have the fitting function
\begin{equation}
  q ( \theta ;a, \mu , \zeta ) \assign \sum_{i=1}^{N_{c}} c_{i} ( a ) \left\{
  \prod_{\alpha =1}^{d} \Phi ( \theta_{\alpha} - \mu_{i  \alpha} , \sigma (
  \zeta_{i  \alpha} ) ) \right\} ,
\end{equation}
where
\begin{eqnarray}
  c_{i} ( a ) & = & \frac{\exp ( a_{i} )}{\sum_{j=1}^{N_{c}} \exp ( a_{j} )} =
  \tmop{softmax} ( i;a ) ; \\
  \sigma ( \zeta_{i  \alpha} ) & = & \ln ( 1+ \exp ( \zeta_{i  \alpha} ) ) , 
\end{eqnarray}
and $a_{i} , \mu_{i  \alpha} , \zeta_{i  \alpha} \in \mathbbm{R}$ for $\forall
i, \forall \alpha$, and
\begin{equation}
  \Phi ( x; \mu , \sigma ) \assign \sqrt{\frac{1}{2  \pi   \sigma^{2}}}   \exp
  \left( - \frac{( x- \mu )^{2}}{2  \sigma^{2}} \right)
\end{equation}
being the Gaussian p.d.f.. The introduction of $\zeta$ is for numerical
consideration, see below.

\subsubsection{Numerical Consideration}

If, in $q$, we regard $w$, $\mu$, and $\sigma$ as independent variables, then
the only singularity appears at $\sigma =0$. Indeed, $\sigma$ appears in
$\Phi$ (as well as the derivatives of $\Phi$) as denominator only, while
others as numerators. However, once doing numerical iterations with a finite
step-length of $\sigma$, the probability of reaching or even crossing $0$
point cannot be surely absent. This is how we may encounter this singularity
in practice.

Introducing the $\zeta$ is our trick of avoiding this singularity. Precisely,
using a singular map that pushes the singularity to infinity solves the
singularity. In this case, using $\tmop{softplus} ( . )$ that pushes $\sigma
=0$ to $\zeta \rightarrow - \infty$, so that, with finite steps of iteration,
singularity (at $- \infty$) cannot be reached.

This trick (i.e. pushing a singularity to infinity) is the same as in avoiding
the horizon-singularity of Schwarzschild solution of black hole.

\subsection{Interpretation}

\subsubsection{As a Mixture Distribution}

$q ( \theta ;a, \mu , \zeta )$ has a probabilistic interpretation.
$\prod_{j=1}^{d} \Phi ( \theta_{j} - \mu_{i j} , \sigma ( \zeta_{i j} ) )$
corresponds to multi-dimensional Gaussian distribution (denote $\mathcal{N}$),
with all dimensions independent with each other. The $\{ c_{i} ( a ) \}$ is a
categorical distribution, randomly choosing the Gaussian distributions. Thus
$q ( \theta ;a, \mu , \zeta )$ is a composition: $\tmop{categorical}  
\rightarrow   \tmop{Gaussian}$. This is the
\href{https://en.wikipedia.org/wiki/Mixture_distribution}{{\tmem{mixture
distribution}}}.

\subsubsection{As a Generalization}

This model can also be interpreted as a direct generalization of
\href{https://arxiv.org/pdf/1601.00670.pdf}{mean-field variational inference}.
Indeed, let $N_{c} =1$, this model reduces to mean-field variational
inference. Remark that mean-field variational inference is a mature algorithm
and has been successfully established on many practical applications.

\subsubsection{As a Neural Network}

\subsection{Marginalization}

This model can be marginalized easily. This then benefits the transfering of
the model components. Precisely, for any dimension-index $\beta$ given, we can
marginalize all other dimensions directly, leaving
\begin{eqnarray}
  q ( \theta_{\beta} ;a, \mu , \zeta ) & = & \prod_{\forall \gamma \neq \beta}
  \int \mathd \theta_{\gamma}   \sum_{i=1}^{N_{c}} c_{i} ( a ) \left\{
  \prod_{\alpha =1}^{d} \Phi ( \theta_{\alpha} ; \mu_{i  \alpha} , \sigma (
  \zeta_{i  \alpha} ) ) \right\} \\
  & = & \sum_{i=1}^{N_{c}} c_{i} ( a )   \Phi ( \theta_{\beta} ; \mu_{i 
  \beta} , \sigma ( \zeta_{i  \beta} ) ) , 
\end{eqnarray}
where employed the normalization of $\Phi$.

\subsection{Loss-Function}

We employ
\href{http://www.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf}{``evidence
of lower bound'' (ELBO)}\footnote{The relation between ELBO and KL-divergence
is that $\tmop{ELBO} =- \tmop{KL} ( q\|p ) + \tmop{Const}$.}. It is ensured to
have a unique global maximum, at which $p ( \theta ;D ) =q ( \theta ;a, \mu ,
\zeta )$.
\[ \tmop{ELBO} ( a, \mu , \zeta ) \assign \mathbbm{E}_{\theta \sim q ( \theta
   ;a, \mu , \zeta )} [ \ln  p ( \theta ;D ) - \ln  q ( \theta ;a, \mu , \zeta
   ) ] . \]
Since there's no compact support for both $p ( \theta ;D )$ and $q ( \theta
;a, \mu , \zeta )$, $\tmop{ELBO}$ is well-defined. The loss-function (or say
loss-function, performance, etc) of the fitting is then defined as
$\mathcal{L} =- \tmop{ELBO}$, i.e.
\[ \mathcal{\mathcal{L} ( a, \mu , \zeta )} =-\mathbbm{E}_{\theta \sim q (
   \theta ;a, \mu , \zeta )} [ \ln  p ( \theta ;D ) - \ln  q ( \theta ;a, \mu
   , \zeta ) ] , \]
or, recall $\mathbbm{H} [ q ] \assign -\mathbbm{E}_{q} [ \ln  q ]$ for any
distribution $q$,
\[ \mathcal{\mathcal{L} ( a, \mu , \zeta )} =-\mathbbm{E}_{\theta \sim q (
   \theta ;a, \mu , \zeta )} [ \ln  p ( \theta ;D ) ] -\mathbbm{H} [ q (
   \theta ;a, \mu , \zeta ) ] . \]

\section{Optimization}

\subsection{ADVI}

Automatic differentation variational inference (ADVI)\footnote{See,
\href{https://arxiv.org/abs/1603.00788}{Kucukelbir, et al, 2016}.} has the
advantage that the variance of its Monte Carlo integral is orderly smaller
than that of black box variational inference (i.e. optimization directly using
ELBO without further reparameterization).

\subsubsection{Derivation}

Precisely, recall $\mathbbm{E}$ for mean value, $\Phi$ for Gaussian p.d.f.,
$\sigma ( . )$ for softplus function, $c ( . )$ for softmax function, and
\begin{equation}
  q ( \theta ;a, \mu , \zeta ) = \sum_{i=1}^{N_{c}} c_{i} ( a )   \Phi (
  \theta ; \mu_{i} , \sigma ( \zeta_{i} ) ) ,
\end{equation}
we have, for any function $f$,
\begin{eqnarray}
  \mathbbm{E}_{q ( \theta ;a, \mu , \zeta )} [ f ( \theta ) ] & = & \int
  \mathd \theta   \sum_{i=1}^{N_{c}} c_{i} ( a )   \Phi ( \theta ; \mu_{i} ,
  \sigma ( \zeta_{i} ) )  f ( \theta ) \\
  & = & \sum_{i=1}^{N_{c}} c_{i} ( a )   \int \mathd \theta   \Phi ( \theta ;
  \mu_{i} , \sigma ( \zeta_{i} ) )  f ( \theta ) \\
  & = & \sum_{i=1}^{N_{c}} c_{i} ( a )  \mathbbm{E}_{\Phi ( \theta ; \mu_{i}
  , \sigma ( \zeta_{i} ) )} [ f ( \theta ) ] . 
\end{eqnarray}
With this general relation, we get
\begin{eqnarray}
  \mathcal{\mathcal{L} ( a, \mu , \zeta )} & = & - \{ \mathbbm{E}_{q ( \theta
  ;a, \mu , \zeta )} [ \ln  p ( \theta ;D ) ] -\mathbbm{E}_{q ( \theta ;a, \mu
  , \zeta )} [ \ln  q ( \theta ;a, \mu , \zeta ) ] \} \\
  & = & - \sum_{i}^{N_{c}} c_{i} ( a )  \mathbbm{E}_{\Phi_{i} ( \theta ;
  \mu_{i} , \sigma ( \zeta_{i} ) )} [ \ln  p ( \theta ;D ) - \ln  q ( \theta
  ;a, \mu , \zeta ) ] 
\end{eqnarray}
Then, for $\forall i=1, \ldots ,N_{c}$, $\forall \alpha =1, \ldots ,N_{d}$,
let
\begin{equation}
  \eta_{\alpha} \assign \frac{\theta_{\alpha} - \mu_{i \alpha}}{\sigma (
  \zeta_{i \alpha} )} ,
\end{equation}
we have
\begin{equation}
  \theta_{\alpha} = \sigma ( \zeta_{i \alpha} )   \eta_{\alpha} + \mu_{i
  \alpha}
\end{equation}
(or $\theta = \sigma ( \zeta_{i} )   \eta + \mu_{i}$ if hide the $\alpha$
index). So, for any $i$-component, we transform
\begin{eqnarray}
  \theta & \rightarrow & \sigma ( \zeta_{i} )   \eta + \mu_{i} ; \\
  \mathbbm{E}_{\Phi ( \theta ; \mu_{i} , \sigma ( \zeta_{i} ) )} [ f ( \theta
  ) ] & \rightarrow & \mathbbm{E}_{\Phi ( \eta ;0,1 )} [ f ( \sigma (
  \zeta_{i} )   \eta + \mu_{i} ) ] , 
\end{eqnarray}
where function $f$ is arbitrary, thus holds for both $\ln  p ( .;D )$ and $\ln
q ( .;a, \mu , \zeta )$.

With this setting, the derivatives to $\mu$ and to $\zeta$ are completely
independent of $\mathbbm{E} [ . ]$. And now, the loss function becomes
\[ \mathcal{L} ( a, \mu , \zeta ) =- \sum_{i}^{N_{c}} c_{i} ( a ) 
   \mathbbm{E}_{\Phi ( \eta ;0,1 )} [ \ln  p ( \sigma ( \zeta_{i} )   \eta +
   \mu_{i} ;D ) - \ln  q ( \sigma ( \zeta_{i} )   \eta + \mu_{i} ;a, \mu ,
   \zeta ) ] . \]

\subsection{Redefination of $\partial \mathcal{L} / \partial a$}

\subsubsection{Gauge Fixing}

Let $\Delta t$ the learning-rate. Then the updation of $a_{i}$ at one
iteration by gradient decent method is
\[ \Delta a_{i} =- \frac{\partial \mathcal{L}}{\partial a_{i}} ( a, \mu ,
   \zeta )   \Delta t. \]
Notice that redefining the $\partial \mathcal{L} / \partial a$ by
\[ \frac{\partial \mathcal{L}}{\partial a_{i}} ( a, \mu , \zeta ) \rightarrow
   \frac{\partial \mathcal{L}}{\partial a_{i}} ( a, \mu , \zeta ) +C, \]
where $C$ can be any constant, leaves the updation of $c_{i} ( a )$ invariant,
since it makes
\[ \Delta a_{i} \rightarrow - \frac{\partial \mathcal{L}}{\partial a_{i}} ( a,
   \mu , \zeta )   \Delta t-C  \Delta t, \]
thus
\[ c_{i} ( a+ \Delta a ) \rightarrow \frac{\exp ( a_{i} + \Delta a_{i} -C 
   \Delta t )}{\sum_{j} \exp ( a_{j} + \Delta a_{j} -C  \Delta t )} =
   \frac{\exp ( a_{i} + \Delta a_{i} )}{\sum_{j} \exp ( a_{j} + \Delta a_{j}
   )} =c_{i} ( a+ \Delta a ) . \]
This $C$ thus provides an additional dof.\footnote{As CL explained, the $c$s
have less dofs as they look, since $\sum_{i}^{N_{c}} c_{i} =1$. This
restriction can provides an additional gauge. And the new dof $C$ fixes this
gauge.} We can tune the value of $C$ so that the updation of $a_{i}$ is
numerically stable. Indeed, let $C$ be the average of $\{ \partial \mathcal{L}
/ \partial a_{i} :i=1, \ldots ,N_{c} \}$, we find a pretty stability of $a$ as
well as a pretty accuracy of $c$ in the iteration process of optimization, as
the experiment on Gaussian mixture model shows.

\subsubsection{Re-scaling of $a$}

There shall be an additional hyper-parameter for the re-scaling of $a$. The
re-scaling factor, constant $r$, redefines
\[ c_{i} ( a ) \assign \tmop{softmax} ( i,r a ) . \]
Tuning this additional hyper-parameter may improve the optimization.

\subsection{Training Strategy}

Generally we hope that the gradient diminishes when and only when the
optimization converges. However, even far from convergence, a tiny $c_{i}$
will diminish all the derivatives in the $i$-component, e.g. derivatives of
$a_{i}$, $\mu_{i \alpha}$, $\zeta_{i \alpha}$, since all these derivatives are
proportional to $c_{i}$.

A proper strategy is setting the re-scaling factor of $a$, the $r$, small
enough (e.g. vanishing) until other variables vary little, and then reset the
$r$ to any value eagered. This can avoid the problem that a pre-mature state
of training may give a tiny $c_{i}$, which then slows down the training of all
variables in the $i$-component.

\subsection{Approximations}

Comparing to the traditional MAP approach, using multi-peak mixture model
makes the $\mathbbm{H} ( q )$ complicated, especially in the optimization
process.

\subsubsection{Entropy Lower Bound}

Consider any mixture distribution with p.d.f. $\sum_{i}^{N_{c}} c_{i}  q_{i}$
where $c$s are the categorical probabilities and $q_{i}$ the p.d.f. of the
component distributions of the mixture.
\[ \mathbbm{H} \left[ \sum_{i}^{N_{c}} c_{i}  q_{i} \right] \geqslant
   \sum_{i}^{N_{c}} c_{i}  \mathbbm{H} [ q_{i} ] . \]
So, if define
\begin{equation}
  \mathcal{\mathcal{L}'} ( a, \mu , \zeta ) \assign - \sum_{i}^{N_{c}}  c_{i}
  ( a )   \{ \mathbbm{E}_{\Phi ( \eta ;0,1 )} [ \ln  p ( \sigma ( \zeta_{i} ) 
  \eta + \mu_{i} ;D ) ] +\mathbbm{H} [ \Phi ( \theta , \mu_{i} , \sigma (
  \zeta_{i} ) ) ] \} ,
\end{equation}
then we have
\[ \mathcal{L}' \geqslant \mathcal{L \geqslant \min ( \mathcal{L} ) >- \infty}
   , \]
thus $\mathcal{L}'$ has an global minimum. In this way, the entropy part
becomes completely analytic (and simple).

However, as experiment on Gaussian mixture model shows, using entropy lower
bound cannot get the enough accuracy as using entropy does.

\subsection{Stochastic Optimization}

\subsubsection{Difference between Bayesian and Traditional Methods}

Suppose, instead of use the whole dataset, we employ mini-batch technique.
Since all data are independent, if suppose that $D_{m}$ is unbiased in $D$,
then we have,
\begin{equation}
  \ln  p ( D| \theta ) = \sum_{D} p ( ( x_{i} ,y_{i} , \sigma_{i} ) | \theta )
  \approx \frac{N_{D}}{N_{m}} \sum_{D_{m}} p ( ( x_{i} ,y_{i} , \sigma_{i} ) |
  \theta ) = \frac{N_{D}}{N_{m}}   \ln  p ( D_{m} | \theta ) .
\end{equation}
Then,
\begin{equation}
  \ln  p ( \theta ;D ) = \ln  p ( D| \theta ) + \ln  p ( \theta ) =
  \frac{N_{D}}{N_{m}}   \ln  p ( D_{m} | \theta ) + \ln  p ( \theta ) ,
\end{equation}
thus as previous
\begin{equation}
  \ln  p ( \theta ;D ) = \frac{N_{D}}{N_{m}} \sum_{( x_{i} ,y_{i} , \sigma_{i}
  ) \in D_{m}} \left\{ - \frac{1}{2} \ln   ( 2  \pi   \sigma_{i}^{2} ) -
  \frac{1}{2}   \left( \frac{y_{i} -f ( x_{i} ; \theta )}{\sigma_{i}}
  \right)^{2} \right\} + \ln  p ( \theta ) .
\end{equation}
In this we meet one of the main differences between the Bayesian and the
traditional. In the traditional method, $N_{D}$ does not matters in training,
being absent in the optimizer. However, in Bayesian, the number of data that
are employed is encoded into Bayesian model, and has to, since the greater
number of data gives more confidence. So, while using stochastic optimization
in Bayesian mode, the factor $N_{D} /N_{m}$ of likelihood has to be taken into
account. We have to know how many data we actually have, thus how confident we
are.

\section{Problems and Solutions}

\subsection{Non-Synchronous Problem}

(XXX see the e-mail.)

\section{Deep Learning}

It cannot solve the vanishing gradient problem of deep neural network, since
this problem is intrinsic to the posterior of deep neural network. Indeed, the
posterior has the shape like $\exp ( -x^{2} / \sigma^{2} )$ with $\sigma
\rightarrow 0$, where $x$ is the variable (argument) of the posterior. It has
a sharp peak, located at a tiny area, with all other region extremely flat.
The problem of find this peak, or equivalently, finding its tiny area, is
intrinsically intractable.

So, even for Bayesian neural network, a layer by layer abstraction along depth
cannot be absent.

\section{Transfer Learning}

Transfer learning demands that the model can be separated so that, say, some
lower level layers can be extracted out and directly transfered to another
model as its lower level layers without any modification on these layers. To
do so, we have to demand that the marginalization of the $q ( \theta ;a, \mu ,
\zeta )$ on some $\theta_{i}$s shall be easy to take. Indeed, the
marginalization of our model is straight forward.

\section{Why not MCMC?}

Instead, the MCMC approximation to posterior cannot be marginalized easily,
and even intractable. So, MCMC approximation cannot provide transfer learning
as we eager. This is the most important reason that we do not prefer MCMC.
Furthermore, MCMC is not greedy enough so that it converges quite slow,
especially in high-dimensional parameter-space.

\end{document}
