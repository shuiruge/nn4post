\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,graphicx,bbm,hyperref}

%%%%%%%%%% Start TeXmacs macros
\catcode`\|=\active \def|{
\fontencoding{T1}\selectfont\symbol{124}\fontencoding{\encodingdefault}}
\newcommand{\assign}{:=}
\newcommand{\mathd}{\mathrm{d}}
\newcommand{\tmcodeinline}[2][]{{\ttfamily{#2}}}
\newcommand{\tmem}[1]{{\em #1\/}}
\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
%%%%%%%%%% End TeXmacs macros

\begin{document}

\section{Notations}

\subsection{Model \& Data}

Let $f ( x; \theta )$ a function of $x$ with parameter $\theta$. Let $y=f ( x;
\theta )$ an observable, thus the observed value obeys a Gaussian
distribution. Let $D$ denote a list of observations, $D \assign \{ ( x_{i}
,y_{i} , \sigma_{i} ) :i=1, \ldots ,N_{D} \}$, wherein $x_{i}$ is the $i$th
input, $y_{i}$ its observed value, and $\sigma_{i}$ the observational error of
$y_{i}$. We may employ mini-batch technique, thus denote $D_{m} \assign \{ (
x_{i} ,y_{i} , \sigma_{i} ) :i=1, \ldots ,N_{m} \} \subset D$ as a mini-batch,
with batch-size $N_{m} \leqslant N_{D}$.

\section{Bayesian}

\subsection{Prior-Posterior Iteration}

\subsection{Bayesian as Information Encoder}

Comparing with the traditional method, what is the advantage of Bayesian way?
The answer is, it encodes more information of data into model. Indeed, it does
not encodes the value of peak of the posterior only, as traditional method
does, but also much more information on the posterior. XXX

\section{Neural Network for Posterior (nn4post)}

\subsection{The Model}

Suppose we have some prior on $\theta$, $p ( \theta )$, we gain the
unormalized posterior $p ( D| \theta )  p ( \theta )$. With $D$ arbitrarily
given, this unormalized posterior is a function of $\theta$, denoted by $p (
\theta ;D )$\footnote{This is why we use `` $;$ '' instead of `` $,$ '',
indicating that $D$ has been (arbitrarily) given and fixed.}.\footnote{The
normalized posterior $p ( \theta |D ) =p ( D| \theta )  p ( \theta ) /p ( D )
=p ( \theta ;D ) /p ( D )$, by Bayes's rule.}

We we are going to do is fit this $p ( \theta ;D )$ by ANN for any given $D$.
To do so, we have to assume that $\tmop{supp} \{ p ( \theta ;D ) \}
=\mathbbm{R}^{d}$ for some $d \in \mathbbm{N}^{+}$ (i.e. has no compact
support) but decrease exponentially fast as $\| \theta \| \rightarrow +
\infty$. With this assumption, $\ln  p ( \theta ;D )$ is well-defined. For
ANN, we propose using Gaussian function as the activation-function. Thus, we
have the fitting function
\[ q ( \theta ;a, \mu , \zeta ) = \sum_{i=1}^{N_{c}} c_{i} ( a ) \left\{
   \prod_{j=1}^{d} \Phi ( \theta_{j} - \mu_{i j} , \sigma ( \zeta_{i j} ) )
   \right\} , \]
where
\begin{eqnarray*}
  c_{i} ( a ) & = & \frac{\exp ( a_{i} )}{\sum_{j=1}^{N} \exp ( a_{j} )} =
  \tmop{softmax} ( i;a ) ;\\
  \sigma ( \zeta_{i j} ) & = & \ln ( 1+ \exp ( \zeta_{i j} ) ) ,
\end{eqnarray*}
and $a_{i} , \mu_{i j} , \zeta_{i j} \in \mathbbm{R}$ for $\forall i, \forall
j$ and
\[ \Phi ( x- \mu , \sigma ) \assign \sqrt{\frac{1}{2  \pi   \sigma^{2}}}  
   \exp \left( - \frac{( x- \mu )^{2}}{2  \sigma^{2}} \right) \]
being the Gaussian PDF. The introduction of $\zeta$ is for numerical
consideration, see below.

\subsubsection{Numerical Consideration}

If, in $q$, we regard $w$, $\mu$, and $\sigma$ as independent variables, then
the only singularity appears at $\sigma =0$. Indeed, $\sigma$ appears in
$\Phi$ (as well as the derivatives of $\Phi$) as denominator only, while
others as numerators. However, once doing numerical iterations with a finite
step-length of $\sigma$, the probability of reaching or even crossing $0$
point cannot be surely absent. This is how we may encounter this singularity
in practice.

Introducing the $\zeta$ is our trick of avoiding this singularity. Precisely,
using a singular map that pushes the singularity to infinity solves the
singularity. In this case, using $\tmop{softplus} ( . )$ that pushes $\sigma
=0$ to $\zeta \rightarrow - \infty$, so that, with finite steps of iteration,
singularity (at $- \infty$) cannot be reached.

This trick (i.e. pushing a singularity to infinity) is the same as in avoiding
the horizon-singularity of Schwarzschild solution of black hole.

\subsection{Interpretation}

\subsubsection{As a Mixture Distribution}

$q ( \theta ;a, \mu , \zeta )$ has a probabilistic interpretation.
$\prod_{j=1}^{d} \Phi ( \theta_{j} - \mu_{i j} , \sigma ( \zeta_{i j} ) )$
corresponds to multi-dimensional Gaussian distribution (denote $\mathcal{N}$),
with all dimensions independent with each other. The $\{ c_{i} ( a ) \}$ is a
categorical distribution, randomly choosing the Gaussian distributions. Thus
$q ( \theta ;a, \mu , \zeta )$ is a composition: $\tmop{categorical}  
\rightarrow   \tmop{Gaussian}$. This is the
\href{https://en.wikipedia.org/wiki/Mixture_distribution}{{\tmem{mixture
distribution}}}.

\subsubsection{As a Generalization}

This model can also be interpreted as a direct generalization of
\href{https://arxiv.org/pdf/1601.00670.pdf}{mean-field variational inference}.
Indeed, let $N_{c} =1$, this model reduces to mean-field variational
inference. Remark that mean-field variational inference is a mature algorithm
and has been successfully established on many practical applications.

\subsubsection{As a Neural Network}

\subsection{Marginalization}

This model can be marginalized easily. This then benefits the transfering of
the model components. Precisely, for any dimension-index $l$ given, we can
marginalize all other dimensions directly, leaving
\begin{eqnarray*}
  q ( \theta_{l} ;a, \mu , \zeta ) & = & \prod_{\forall i \neq l} \int \mathd
  \theta_{i}   \sum_{i=1}^{N_{c}} c_{i} ( a ) \left\{ \prod_{j=1}^{d} \Phi (
  \theta_{j} - \mu_{i j} , \sigma ( \zeta_{i j} ) ) \right\}\\
  & = & \sum_{i=1}^{N_{c}} c_{i} ( a )   \Phi ( \theta_{l} - \mu_{i l} ,
  \sigma ( \zeta_{i l} ) ) ,
\end{eqnarray*}
where employed the normalization of $\Phi$.

\subsection{Loss-Function}

We use
\href{http://www.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf}{``evidence
of lower bound'' (ELBO)} as loss. It is ensured to have a unique global
minimal, at which $p ( \theta ;D ) =q ( \theta ;a, \mu , \zeta )$.
\begin{eqnarray*}
  \tmop{ELBO} ( a, \mu , \zeta ) & \assign & \mathbbm{E}_{\theta \sim q (
  \theta ;a, \mu , \zeta )} [ \ln  p ( \theta ;D ) - \ln  q ( \theta ;a, \mu ,
  \zeta ) ]\\
  & \approx & \left( \frac{1}{n}   \sum_{\theta_{( s )}} \right) \{ \ln  p (
  \theta_{( s )} ;D ) - \ln  q ( \theta_{( s )} ;a, \mu , \zeta ) \} ,
\end{eqnarray*}
where $\{ \theta_{( s )} : s=1, \ldots ,n \}$ is sampled from $q ( \theta ;a,
\mu , \zeta )$ as a distribution. Since there's no compact support for both $p
( \theta ;D )$ and $q ( \theta ;a, \mu , \zeta )$, $\tmop{ELBO}$ is
well-defined, as the loss-function (or say loss-function, performance, etc) of
the fitting.

\section{Stochastic Optimization}

\subsection{Difference between Bayesian and Traditional Methods}

Suppose, instead of use the whole dataset, we employ mini-batch technique.
Since all data are independent, if suppose that $D_{m}$ is unbiased in $D$,
then we have,
\[ \ln  p ( D| \theta ) = \sum_{D} p ( ( x_{i} ,y_{i} , \sigma_{i} ) | \theta
   ) \approx \frac{N_{D}}{N_{m}} \sum_{D_{m}} p ( ( x_{i} ,y_{i} , \sigma_{i}
   ) | \theta ) = \frac{N_{D}}{N_{m}}   \ln  p ( D_{m} | \theta ) . \]
Then,
\[ \ln  p ( \theta ;D ) = \ln  p ( D| \theta ) + \ln  p ( \theta ) =
   \frac{N_{D}}{N_{m}}   \ln  p ( D_{m} | \theta ) + \ln  p ( \theta ) , \]
thus as previous
\[ \ln  p ( \theta ;D ) = \frac{N_{D}}{N_{m}} \sum_{( x_{i} ,y_{i} ,
   \sigma_{i} ) \in D_{m}} \left\{ - \frac{1}{2} \ln   ( 2  \pi  
   \sigma_{i}^{2} ) - \frac{1}{2}   \left( \frac{y_{i} -f ( x_{i} ; \theta
   )}{\sigma_{i}} \right)^{2} \right\} + \ln  p ( \theta ) . \]
In this we meet one of the main differences between the Bayesian and the
traditional. In the traditional method, $N_{D}$ does not matters in training,
being absent in the optimizer. However, in Bayesian, the number of data that
are employed is encoded into Bayesian model, and has to, since the greater
number of data gives more confidence. So, while using stochastic optimization
in Bayesian mode, the factor $N_{D} /N_{m}$ of likelihood has to be taken into
account. We have to know how many data we actrually have, thus how confident
we are.

\section{ADVI}

Automatic differentation variational inference (ADVI)\footnote{See,
\href{https://arxiv.org/abs/1603.00788}{Kucukelbir, et al, 2016}.} has the
advantage that the variance of its Monte Carlo integral is orderly smaller
than that of black box variational inference (i.e. optimization directly using
ELBO without further reparameterization).

Precisely, let $\mathbbm{E}$ for mean value, $\mathbbm{H}$ for shannon
entropy, $\Phi$ for Gaussian, and $\sigma ( . )$ for softplus function. By
\[ q ( \theta ;a, \mu , \zeta ) = \sum_{i=1}^{N_{c}} w_{i} ( a )   \Phi (
   \theta ; \mu_{i} , \sigma ( \zeta_{i} ) ) , \]
we have
\begin{eqnarray*}
  \tmop{ELBO} & = & \mathbbm{E}_{q ( \theta ;a, \mu , \zeta )} [ \ln  p (
  \theta ;D ) ] +\mathbbm{H} [ q ( \theta ;a, \mu , \zeta ) ]\\
  & = & \sum_{i}^{N_{c}} w_{i} ( a )  \mathbbm{E}_{\Phi_{i} ( \theta ;
  \mu_{i} , \sigma ( \zeta_{i} ) )} [ \ln  p ( \theta ;D ) ] +\mathbbm{H} [ q
  ( \theta ;a, \mu , \zeta ) ]\\
  & =: & E_{1} +E_{2} .
\end{eqnarray*}
($E_{2}$ is analytic and independent of $p ( \theta ;D )$, so we leave it for
later.) Then, for $\forall i=1, \ldots ,N_{c}$, $\forall \alpha =1, \ldots
,N_{d}$, let
\[ \eta_{\alpha} \assign \frac{\theta_{\alpha} - \mu_{i \alpha}}{\sigma (
   \zeta_{i \alpha} )} , \]
we have $\theta_{\alpha} = \sigma ( \zeta_{i \alpha} )   \eta_{\alpha} +
\mu_{i \alpha}$ ($\theta = \sigma ( \zeta_{i} )   \eta + \mu_{i}$ if hides the
$\alpha$ index). So, for any $i$-components in the $E_{1}$ of $\tmop{ELBO}$,
we transform
\[ \mathbbm{E}_{\Phi ( \theta ; \mu_{i} , \sigma ( \zeta_{i} ) )} [ \ln  p (
   \theta ;D ) ] =\mathbbm{E}_{\Phi ( \eta ;0,1 )} [ \ln  p ( \sigma (
   \zeta_{i} )   \eta + \mu_{i} ;D ) ] . \]
Thus, we have derivatives
\begin{eqnarray*}
  \frac{\partial E_{1}}{\partial \mu_{i \alpha}} & = & w_{i} ( a ) 
  \mathbbm{E}_{\Phi ( \eta ;0,1 )} \left[ \frac{\partial \ln  p}{\partial
  \theta_{\alpha}} ( \theta = \sigma ( \zeta_{i} )   \eta + \mu_{i} ;D )
  \right] ;\\
  \frac{\partial E_{1}}{\partial \mu_{i \alpha}} & = & w_{i} ( a ) 
  \mathbbm{E}_{\Phi ( \eta ;0,1 )} \left[ \frac{\partial \ln  p}{\partial
  \theta_{\alpha}} ( \theta = \sigma ( \zeta_{i} )   \eta + \mu_{i} ;D )  
  \eta_{\alpha} \right] .
\end{eqnarray*}
So, for these two, value of $\ln  p ( \theta ;D )$ is regardless.

\section{Computational Resource of Training}

Recall that $d$ denotes the dimension of $\theta$, the parameter of model $f (
x; \theta )$; $N_{c}$ denotes the number of categories in the mixture
distribution; $N_{D}$ the number of data.

The dependence of computational resource on $N_{D}$ is intactable, since this
dependence is determined by the inner complexity of $f ( x; \theta )$. Thus,
we shall fix this $N_{D}$ or just omit it by introducing mini-batch technique.

\subsection{At Each Iteration}

\subsubsection{Overview}

At each step of iteration of optimizer (e.g.
\tmcodeinline[cpp]{GradientDescentOptimizer}), the computational resources
spent on time, for traditional maxima a posterior, variational inference with
mean-field approximation, and neural network for posterior respectively:
\begin{eqnarray*}
  \text{MAP} & = & \Theta ( d ) ;\\
  \text{Mean-Field VI} & = & \Theta ( d ) ;\\
  \text{nn4post} & = & \Theta ( N_{c}  d ) .
\end{eqnarray*}

\subsubsection{Traditional MAP}

At each step of iteration of optimizer (e.g.
\tmcodeinline[cpp]{GradientDescentOptimizer}), the computational resource
spent on time is of $\Theta ( d )$, i.e. computing the partial derivative
values of loss-function by model parameters $\{ \theta_{j} :j=1,2, \ldots ,d
\}$.

\subsubsection{Variational Inference with Mean-Field Approximation}

At each step of iteration of optimizer (e.g.
\tmcodeinline[cpp]{GradientDescentOptimizer}), the computational resource
spent on time is of $\Theta ( 2 d ) = \Theta ( d )$, i.e. computing the
partial derivative values of loss-function by each parameter of mean-field
approximation $\{ ( \mu_{j} , \sigma_{j} ) :j=1,2, \ldots ,d \}$.

\subsubsection{Neural Network for Posterior}

At each step of iteration of optimizer (e.g.
\tmcodeinline[cpp]{GradientDescentOptimizer}), the computational resource
spent on time is of $\Theta ( N_{c} +2 N_{c}  d ) = \Theta ( N_{c}  d
)$\footnote{Herein we have supposed that $d \gg 1$, which is quite
practical.}, i.e. computing the partial derivative values of loss-function by
each paramter of mean-field approximation
\[ \{ ( a_{i} , \mu_{i j} , \zeta_{i j} ) :i=1,2, \ldots ,N_{c} ;j=1,2, \ldots
   ,d \} . \]

\subsection{Essential Number of Iterations}

The essential number of iterations of optimizer depends both on $N_{c}$ and
$d$, and increasing either $N_{c}$ or $d$ will also increase it.

Indeed, when increasing $d$, the searching path of peaks in the paramter-space
can oscillate along more dimensions, this makes the path longer.

And, when increasing $N_{c}$, the optimizer needs more steps of iterations for
tuning the relative ratios between the $a_{i}$s, while in the case of
mean-field approximation where $N_{c} =1$, there's no need of such tuning.
This effect can be visualized by the figure 1, wherein notice that, since the
two loss are closed in the tail, it hints that $N_{c} =1$ is the intrinsic
number of peaks of the posterior.

\begin{figure}[h]
  \resizebox{900px}{600px}{\includegraphics{nn4post-1.eps}}
  \caption{\label{figure: 1}The orange line represents $N_{c} =1$ and the red
  $N_{c} =2$ (``\tmcodeinline[cpp]{a\_comp\_i}'' represents $a_{i}$). The
  first converges faster than the later. And precisely as it shows, the case
  $N_{c} =2$ needs $500$ steps of iterations to tune the $a_{1}$ and $a_{2}$
  so that only one peak is essentially left, and it is just around $500$ steps
  of iterations that the two losses get together. (For the source code, see
  \tmcodeinline[cpp]{'nn4post/tests/shallow\_neural\_network.py'}.)}
\end{figure}

\subsection{Batch-Size}

The batch-size needed for variational inference (including both this model and
the mean-field approximiation) is generally greater than that for
non-Bayesian. This is an experimental result (on MNIST dataset), but what is
the reason?

\section{When \& How to Use?}

As the figure \ref{figure: 1} hints, employing a large $N_{c}$ will
unnecessarily waste computational resource. So, instead we'd better try $N_{c}
=1,2,3, \ldots$ one by one, until increasing $N_{c}$ cannot reduce the loss
apparently. At this situation, e.g. $N_{c} =n$ for some $n$, it hints that the
posterior we are fitting has only $n$ apparent peaks. This reveals the
intrinsic nature of the posterior, and we shall stop increasing $N_{c}$ any
more, stop wasting the computational resource.\footnote{(A proposal:) Or
iteratively? That is, first training by $N_{c} =1$; when loss becomes stable
after a period of training, add a new peak, so that $N_{c} =1 \rightarrow 2$;
then, when loss becomes stable again after a new period of training, add a new
peak, so that $N_{c} =2 \rightarrow 3$; repeating. Question: if so, then what
is the initial value of $a$ of the newly added peak? (Being $a_{\max}$?)}

\section{Deep Learning}

It cannot solve the vanishing gradient problem of deep neural network, since
this problem is intrinsic to the posterior of deep neural network. Indeed, the
posterior has the shape like $\exp ( -x^{2} / \sigma^{2} )$ with $\sigma
\rightarrow 0$, where $x$ is the variable (argument) of the posterior. It has
a sharp peak, located at a tiny area, with all other region extremely flat.
The problem of find this peak, or equivalently, findng its tiny area, is
intrinsically intactable.

So, even for Bayesian neural network, a layer by layer abstraction along depth
cannot be absent.

\section{Transfer Learning}

\section{Why not MCMC?}

\section{Drafts}

The problem of optimization appearing in
\tmcodeinline[shell]{gaussian\_mixture\_model.py} may be caused by the
``non-normalization'' of paramter-space,XXX

A possible strategy is iteratively adding peaks. While doing so, at each
iteration, first normalize the paramter-space by the positions of the peaks
found at the previous iteration.XXX

\end{document}
