\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,graphicx,bbm,hyperref}

%%%%%%%%%% Start TeXmacs macros
\catcode`\|=\active \def|{
\fontencoding{T1}\selectfont\symbol{124}\fontencoding{\encodingdefault}}
\newcommand{\assign}{:=}
\newcommand{\tmcodeinline}[2][]{{\ttfamily{#2}}}
\newcommand{\tmem}[1]{{\em #1\/}}
\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
%%%%%%%%%% End TeXmacs macros

\begin{document}

\section{Notations}

\subsection{Model \& Data}

Let $f ( x; \theta )$ a function of $x$ with parameter $\theta$. Let $y=f ( x;
\theta )$ an observable, thus the observed value obeys a Gaussian
distribution. Let $D$ denote a list of observations, $D \assign \{ ( x_{i}
,y_{i} , \sigma_{i} ) :i=1, \ldots ,N_{D} \}$, wherein $x_{i}$ is the $i$th
input, $y_{i}$ its observed value, and $\sigma_{i}$ the observational error of
$y_{i}$. We may employ mini-batch technique, thus denote $D_{m} \assign \{ (
x_{i} ,y_{i} , \sigma_{i} ) :i=1, \ldots ,N_{m} \} \subset D$ as a mini-batch,
with batch-size $N_{m} \leqslant N_{D}$.

\section{Preliminary}

\subsection{The Bayesian Formula}

By Bayes's formula, the posterior can be computed by a valid set of likelihood
and prior, as
\[ \ln  p ( \theta |D ) = \ln  p ( D| \theta ) + \ln  p ( \theta ) - \ln  p (
   D ) . \]

\subsection{The General Form of Likelihood}

It's very practical to propose that the data are all independent and Gaussian.
Indeed, since observations are generally independently taken, this gives the
independence; observations with many times of repeatations gives Gaussianity.
Then, the likelihood becomes
\begin{eqnarray*}
  \ln  p ( D| \theta ) & = & \ln \left( \prod_{i=1}^{N_{D}} \frac{1}{\sqrt{2 
  \pi   \sigma_{i}^{2}}}   \exp \left\{ - \frac{1}{2}   \left( \frac{y_{i} -f
  ( x_{i} ; \theta )}{\sigma_{i}} \right)^{2} \right\} \right)\\
  & = & \sum_{i=1}^{N_{D}} \left\{ - \frac{1}{2} \ln   ( 2  \pi  
  \sigma_{i}^{2} ) - \frac{1}{2}   \left( \frac{y_{i} -f ( x_{i} ; \theta
  )}{\sigma_{i}} \right)^{2} \right\} .
\end{eqnarray*}

\subsection{Assumption on Prior}

Assumption on prior shall take lots of cares and considerations. A bad (or
unreasonable) choice of prior ruins all.

\subsubsection{Priors in Bayesian Neural Network}

As an example, consider Bayesian neural network. Herein, the prior on weights
and that on biases are intrinsically different. For prior on weights, we
employ Gaussian, and then averaged; and for that on biases, we take uniform
instead. This can be illustrated in the section 2.1.1 of Neal (1995) in
principle, and in
\href{http://neuralnetworksanddeeplearning.com/chap3.html#regularization}{section
2.3.1 (``Regularization'') of Nealson (eq. (85) therein)} in practice.

\subsection{Bayesian Inference}

Sample $m$ samples from $p ( \theta |D )$, $\{ \theta_{( s )} :s=1, \ldots ,m
\}$. Thus, the Bayesian inference gives prediction from $x$ to $y$ as
\begin{eqnarray*}
  \hat{y} & = & \mathbbm{E}_{\theta \sim p ( \theta |D )} [ f ( x; \theta )
  ]\\
  & \approx & \left( \frac{1}{m} \sum_{s=1}^{m} \right) f ( x; \theta_{( s )}
  ) .
\end{eqnarray*}

\subsection{Bayesian as Information Encoder}

Comparing with the traditional method, what is the advantage of Bayesian way?
The answer is, it encodes more information of data into model. Indeed, it does
not encodes the value of peak of the posterior only, as traditional method
does, but also much more information on the posterior. XXX

\section{Neural Network for Posterior (nn4post)}

\subsection{The Model}

Suppose we have some prior on $\theta$, $p ( \theta )$, we gain the
unormalized posterior $p ( D| \theta )  p ( \theta )$. With $D$ arbitrarily
given, this unormalized posterior is a function of $\theta$, denoted by $p (
\theta ;D )$\footnote{This is why we use `` $;$ '' instead of `` $,$ '',
indicating that $D$ has been (arbitrarily) given and fixed.}.\footnote{The
normalized posterior $p ( \theta |D ) =p ( D| \theta )  p ( \theta ) /p ( D )
=p ( \theta ;D ) /p ( D )$, by Bayes's rule.}

We we are going to do is fit this $p ( \theta ;D )$ by ANN for any given $D$.
To do so, we have to assume that $\tmop{supp} \{ p ( \theta ;D ) \}
=\mathbbm{R}^{d}$ for some $d \in \mathbbm{N}^{+}$ (i.e. has no compact
support) but decrease exponentially fast as $\| \theta \| \rightarrow +
\infty$. With this assumption, $\ln  p ( \theta ;D )$ is well-defined. For
ANN, we propose using Gaussian function as the activation-function. Thus, we
have the fitting function
\[ q ( \theta ;a, \mu , \zeta ) = \sum_{i=1}^{N_{c}} w_{i} ( a ) \left\{
   \prod_{j=1}^{d} \Phi ( \theta_{j} - \mu_{i j} , \sigma ( \zeta_{i j} ) )
   \right\} , \]
where
\begin{eqnarray*}
  w_{i} ( a ) & = & \frac{\exp ( a_{i} )}{\sum_{j=1}^{N} \exp ( a_{j} )} =
  \tmop{softmax} ( i;a ) ;\\
  \sigma ( \zeta_{i j} ) & = & \ln ( 1+ \exp ( \zeta_{i j} ) ) ,
\end{eqnarray*}
and $a_{i} , \mu_{i j} , \zeta_{i j} \in \mathbbm{R}$ for $\forall i, \forall
j$ and
\[ \Phi ( x- \mu , \sigma ) \assign \sqrt{\frac{1}{2  \pi   \sigma^{2}}}  
   \exp \left( - \frac{( x- \mu )^{2}}{2  \sigma^{2}} \right) \]
being the Gaussian PDF. The introduction of $\zeta$ is for numerical
consideration, see below.

\subsubsection{Numerical Consideration}

If, in $q$, we regard $w$, $\mu$, and $\sigma$ as independent variables, then
the only singularity appears at $\sigma =0$. Indeed, $\sigma$ appears in
$\Phi$ (as well as the derivatives of $\Phi$) as denominator only, while
others as numerators. However, once doing numerical iterations with a finite
step-length of $\sigma$, the probability of reaching or even crossing $0$
point cannot be surely absent. This is how we may encounter this singularity
in practice.

Introducing the $\zeta$ is our trick of avoiding this singularity. Precisely,
using a singular map that pushes the singularity to infinity solves the
singularity. In this case, using $\tmop{softplus} ( . )$ that pushes $\sigma
=0$ to $\zeta \rightarrow - \infty$, so that, with finite steps of iteration,
singularity (at $- \infty$) cannot be reached.

This trick (i.e. pushing a singularity to infinity) is the same as in avoiding
the horizon-singularity of Schwarzschild solution of black hole.

\subsection{Interpretation}

\subsubsection{As a Mixture Distribution}

$q ( \theta ;a, \mu , \zeta )$ has a probablitic interpretation.
$\prod_{j=1}^{d} \Phi ( \theta_{j} - \mu_{i j} , \sigma ( \zeta_{i j} ) )$
corresponds to multi-dimensional Gaussian distribution (denote $\mathcal{N}$),
with all dimensions independent with each other. The $\{ w_{i} ( a ) \}$ is a
categorical distribution, randomly choosing the Gaussian distributions. Thus
$q ( \theta ;a, \mu , \zeta )$ is a composition: $\tmop{categorical}  
\rightarrow   \tmop{Gaussian}$. This is the
\href{https://en.wikipedia.org/wiki/Mixture_distribution}{{\tmem{mixture
distribution}}}.

\subsubsection{As a Generalization}

This model can also be intrepreted as a direct generalization of
\href{https://arxiv.org/pdf/1601.00670.pdf}{mean-field variational inference}.
Indeed, let $N_{c} =1$, this model reduces to mean-field variational
inference. Remark that mean-field variational inference is a mature algorithm
and has been sucessfully established on many practical applications.

\subsection{Loss-Function}

We use
\href{http://www.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf}{``evidence
of lower bound'' (ELBO)} as loss. It is ensured to have a unique global
minimal, at which $p ( \theta ;D ) =q ( \theta ;a, \mu , \zeta )$.
\begin{eqnarray*}
  \tmop{ELBO} ( a, \mu , \zeta ) & \assign & \mathbbm{E}_{\theta \sim q (
  \theta ;w,b )} [ \ln  p ( \theta ;D ) - \ln  q ( \theta ;a, \mu , \zeta )
  ]\\
  & \approx & \left( \frac{1}{n}   \sum_{\theta^{( s )}} \right) \{ \ln  p (
  \theta_{( s )} ;D ) - \ln  q ( \theta_{( s )} ;a, \mu , \zeta ) \} ,
\end{eqnarray*}
where $\{ \theta_{( s )} : s=1, \ldots ,n \}$ is sampled from $q ( \theta ;a,
\mu , \zeta )$ as a distribution. Since there's no compact support for both $p
( \theta ;D )$ and $q ( \theta ;a, \mu , \zeta )$, $\tmop{ELBO}$ is
well-defined, as the loss-function (or say loss-function, performance, etc) of
the fitting.

\section{Stochastic Optimization}

Suppose, instead of use the whole dataset, we employ mini-batch technique. Let
$D_{m}$ denotes the mini-batch with batch-size $N_{m}$. With this, we have
approximation
\[ \ln  p ( \theta ;D ) \approx \frac{N_{D}}{N_{m}}   \ln  p ( \theta ;D_{m} )
   . \]
Let $q_{m} ( \theta ;a, \mu , \zeta )$ the function that fits the $p ( \theta
;D_{m} )$, thus we would expect
\[ q ( \theta ;a, \mu , \zeta ) \approx [ q_{m} ( \theta ;a, \mu , \zeta )
   ]^{N_{D} /N_{m}} . \]
XXX

\section{Computational Resource of Training}

Recall that $d$ denotes the dimension of $\theta$, the parameter of model $f (
x; \theta )$; $N_{c}$ denotes the number of categories in the mixture
distribution; $N_{D}$ the number of data.

The dependence of computational resource on $N_{D}$ is intactable, since this
dependence is determined by the inner complexity of $f ( x; \theta )$. Thus,
we shall fix this $N_{D}$ or just omit it by introducing mini-batch technique.

\subsection{At Each Iteration}

\subsubsection{Overview}

At each step of iteration of optimizer (e.g.
\tmcodeinline[cpp]{GradientDescentOptimizer}), the computational resources
spent on time, for traditional maxima a posterior, variational inference with
mean-field approximation, and neural network for posterior respectively:
\begin{eqnarray*}
  \text{MAP} & = & \Theta ( d ) ;\\
  \text{Mean-Field VI} & = & \Theta ( d ) ;\\
  \text{nn4post} & = & \Theta ( N_{c}  d ) .
\end{eqnarray*}

\subsubsection{Traditional MAP}

At each step of iteration of optimizer (e.g.
\tmcodeinline[cpp]{GradientDescentOptimizer}), the computational resource
spent on time is of $\Theta ( d )$, i.e. computing the partial derivative
values of loss-function by model paramters $\{ \theta_{j} :j=1,2, \ldots ,d
\}$.

\subsubsection{Variational Inference with Mean-Field Approximation}

At each step of iteration of optimizer (e.g.
\tmcodeinline[cpp]{GradientDescentOptimizer}), the computational resource
spent on time is of $\Theta ( 2 d ) = \Theta ( d )$, i.e. computing the
partial derivative values of loss-function by each paramter of mean-field
approximation $\{ ( \mu_{j} , \sigma_{j} ) :j=1,2, \ldots ,d \}$.

\subsubsection{Neural Network for Posterior}

At each step of iteration of optimizer (e.g.
\tmcodeinline[cpp]{GradientDescentOptimizer}), the computational resource
spent on time is of $\Theta ( N_{c} +2 N_{c}  d ) = \Theta ( N_{c}  d
)$\footnote{Herein we have supposed that $d \gg 1$, which is quite
practical.}, i.e. computing the partial derivative values of loss-function by
each paramter of mean-field approximation
\[ \{ ( a_{i} , \mu_{i j} , \zeta_{i j} ) :i=1,2, \ldots ,N_{c} ;j=1,2, \ldots
   ,d \} . \]

\subsection{Essential Number of Iterations}

The essential number of iterations of optimizer depends both on $N_{c}$ and
$d$, and increasing either $N_{c}$ or $d$ will also increase it.

Indeed, when increasing $d$, the searching path of peaks in the paramter-space
can oscillate along more dimensions, this makes the path longer.

And, when increasing $N_{c}$, the optimizer needs more steps of iterations for
tuning the relative ratios between the $a_{i}$s, while in the case of
mean-field approximation where $N_{c} =1$, there's no need of such tuning.
This effect can be visualized by the figure 1, wherein notice that, since the
two loss are closed in the tail, it hints that $N_{c} =1$ is the intrinsic
number of peaks of the posterior.

\begin{figure}[h]
  \resizebox{900px}{600px}{\includegraphics{nn4post-1.eps}}
  \caption{\label{figure: 1}The orange line represents $N_{c} =1$ and the red
  $N_{c} =2$ (``\tmcodeinline[cpp]{a\_comp\_i}'' represents $a_{i}$). The
  first converges faster than the later. And precisely as it shows, the case
  $N_{c} =2$ needs $500$ steps of iterations to tune the $a_{1}$ and $a_{2}$
  so that only one peak is essentially left, and it is just around $500$ steps
  of iterations that the two losses get together. (For the source code, see
  \tmcodeinline[cpp]{'nn4post/tests/shadow\_neural\_network.py'}.)}
\end{figure}

\subsection{Batch-Size}

The batch-size needed for variational inference (including both this model and
the mean-field approximiation) is generally greater than that for
non-Bayesian. This is an experimental result (on MNIST dataset), but what is
the reason?

\section{When \& How to Use?}

As the figure \ref{figure: 1} hints, employing a large $N_{c}$ will
unnecessarily waste computational resource. So, instead we'd better try $N_{c}
=1,2,3, \ldots$ one by one, until increasing $N_{c}$ cannot reduce the loss
apparently. At this situation, e.g. $N_{c} =n$ for some $n$, it hints that the
posterior we are fitting has only $n$ apparent peaks. This reveals the
intrinsic nature of the posterior, and we shall stop increasing $N_{c}$ any
more, stop wasting the computational resource.\footnote{(A proposal:) Or
iteratively? That is, first training by $N_{c} =1$; when loss becomes stable
after a period of training, add a new peak, so that $N_{c} =1 \rightarrow 2$;
then, when loss becomes stable again after a new period of training, add a new
peak, so that $N_{c} =2 \rightarrow 3$; repeating. Question: if so, then what
is the initial value of $a$ of the newly added peak? (Being $a_{\max}$?)}

\section{Deep Learning}

It cannot solve the vanishing gradient problem of deep neural network, since
this problem is intrinsic to the posterior of deep neural network. Indeed, the
posterior has the shape like $\exp ( -x^{2} / \sigma^{2} )$ with $\sigma
\rightarrow 0$, where $x$ is the variable (argument) of the posterior. It has
a sharp peak, located at a tiny area, with all other region extremely flat.
The problem of find this peak, or equivalently, findng its tiny area, is
intrinsically intactable.

So, even for Bayesian neural network, a layer by layer abstraction along depth
cannot be absent.

\section{Problems}

\subsection{Generalization Problems}

\subsubsection{From Mini-Batch to the Whole Dataset}

XXX $q ( \theta ;a, \mu , \zeta ) \rightarrow [ q ( \theta ;a, \mu , \zeta )
]^{N_{D} /N_{m}}$, this will be intactable if $q ( \theta ;a, \mu , \zeta )$
involves summation.

\subsubsection{Model Transfer}

Since all components of $\theta$ in $q ( \theta ;a, \mu , \zeta )$ are not
independent, transfering the model by copying its subgraph (e.g. several based
layers) and then attaching directly to other model's graph cannot be taken.
XXX

\subsubsection{A Solution: Mean-Field Approximation}

I mean, not the Gaussian $q ( \theta_{i} , \lambda_{i} )$ where $\lambda$s
denotes arbitrary parameters, but a general form of it. However, the $q (
\theta ; \lambda )$ can be decomposited as $q ( \theta ; \lambda ) = \prod_{i}
q ( \theta_{i} ; \lambda_{i} )$. This solves the provious two problems in one
go. XXX

\end{document}
