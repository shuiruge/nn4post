\documentclass{article}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,bbm,hyperref}

%%%%%%%%%% Start TeXmacs macros
\catcode`\|=\active \def|{
\fontencoding{T1}\selectfont\symbol{124}\fontencoding{\encodingdefault}}
\newcommand{\assign}{:=}
\newcommand{\mathd}{\mathrm{d}}
\newcommand{\tmem}[1]{{\em #1\/}}
\newcommand{\tmop}[1]{\ensuremath{\operatorname{#1}}}
%%%%%%%%%% End TeXmacs macros

\begin{document}

\section{Notations}

\subsection{Model \& Data}

Let $f ( x; \theta )$ a function of $x$ with parameter $\theta$. Let $y=f ( x;
\theta )$ an observable, thus the observed value obeys a Gaussian
distribution. Let $D$ denotes a set of observations, $D \assign \{ ( x_{i}
,y_{i} , \sigma_{i} ) :i=1, \ldots ,N_{D} \}$, wherein $x_{i}$ is the $i$th
input, $y_{i}$ its observed value, and $\sigma_{i}$ the observational error of
$y_{i}$. We may employ mini-batch technique, thus denote $D_{m} \assign \{ (
x_{i} ,y_{i} , \sigma_{i} ) :i=1, \ldots ,N_{m} \} \subset D$ as a mini-batch,
with batch-size $N_{m} \leqslant N_{D}$.

\section{Bayesian}

\subsection{Prior-Posterior Iteration}

\subsection{Bayesian as Information Encoder}

Comparing with the traditional method, what is the advantage of Bayesian way?
The answer is, it encodes more information of data into model. Indeed, it does
not encodes the value of peak of the posterior only, as traditional method
does, but also much more information on the posterior. XXX

\section{Neural Network for Posterior (nn4post)}

\subsection{The Model}

Suppose we have some prior on $\theta$, $p ( \theta )$, we gain the
unormalized posterior $p ( D| \theta )  p ( \theta )$. With $D$ arbitrarily
given, this unormalized posterior is a function of $\theta$, denoted by $p (
\theta ;D )$\footnote{This is why we use `` $;$ '' instead of `` $,$ '',
indicating that $D$ has been (arbitrarily) given and fixed.}.\footnote{The
normalized posterior $p ( \theta |D ) =p ( D| \theta )  p ( \theta ) /p ( D )
=p ( \theta ;D ) /p ( D )$, by Bayes's rule.}

We we are going to do is fit this $p ( \theta ;D )$ by ANN for any given $D$.
To do so, we have to assume that $\tmop{supp} \{ p ( \theta ;D ) \}
=\mathbbm{R}^{d}$ for some $d \in \mathbbm{N}^{+}$ (i.e. has no compact
support) but decrease exponentially fast as $\| \theta \| \rightarrow +
\infty$. With this assumption, $\ln  p ( \theta ;D )$ is well-defined. For
ANN, we propose using Gaussian function as the activation-function. Thus, we
have the fitting function
\[ q ( \theta ;a, \mu , \zeta ) = \sum_{i=1}^{N_{c}} c_{i} ( a ) \left\{
   \prod_{\alpha =1}^{d} \Phi ( \theta_{\alpha} - \mu_{i  \alpha} , \sigma (
   \zeta_{i  \alpha} ) ) \right\} , \]
where
\begin{eqnarray*}
  c_{i} ( a ) & = & \frac{\exp ( a_{i} )}{\sum_{j=1}^{N} \exp ( a_{j} )} =
  \tmop{softmax} ( i;a ) ;\\
  \sigma ( \zeta_{i  \alpha} ) & = & \ln ( 1+ \exp ( \zeta_{i  \alpha} ) ) ,
\end{eqnarray*}
and $a_{i} , \mu_{i  \alpha} , \zeta_{i  \alpha} \in \mathbbm{R}$ for $\forall
i, \forall \alpha$ and
\[ \Phi ( x; \mu , \sigma ) \assign \sqrt{\frac{1}{2  \pi   \sigma^{2}}}  
   \exp \left( - \frac{( x- \mu )^{2}}{2  \sigma^{2}} \right) \]
being the Gaussian PDF. The introduction of $\zeta$ is for numerical
consideration, see below.

\subsubsection{Numerical Consideration}

If, in $q$, we regard $w$, $\mu$, and $\sigma$ as independent variables, then
the only singularity appears at $\sigma =0$. Indeed, $\sigma$ appears in
$\Phi$ (as well as the derivatives of $\Phi$) as denominator only, while
others as numerators. However, once doing numerical iterations with a finite
step-length of $\sigma$, the probability of reaching or even crossing $0$
point cannot be surely absent. This is how we may encounter this singularity
in practice.

Introducing the $\zeta$ is our trick of avoiding this singularity. Precisely,
using a singular map that pushes the singularity to infinity solves the
singularity. In this case, using $\tmop{softplus} ( . )$ that pushes $\sigma
=0$ to $\zeta \rightarrow - \infty$, so that, with finite steps of iteration,
singularity (at $- \infty$) cannot be reached.

This trick (i.e. pushing a singularity to infinity) is the same as in avoiding
the horizon-singularity of Schwarzschild solution of black hole.

\subsection{Interpretation}

\subsubsection{As a Mixture Distribution}

$q ( \theta ;a, \mu , \zeta )$ has a probabilistic interpretation.
$\prod_{j=1}^{d} \Phi ( \theta_{j} - \mu_{i j} , \sigma ( \zeta_{i j} ) )$
corresponds to multi-dimensional Gaussian distribution (denote $\mathcal{N}$),
with all dimensions independent with each other. The $\{ c_{i} ( a ) \}$ is a
categorical distribution, randomly choosing the Gaussian distributions. Thus
$q ( \theta ;a, \mu , \zeta )$ is a composition: $\tmop{categorical}  
\rightarrow   \tmop{Gaussian}$. This is the
\href{https://en.wikipedia.org/wiki/Mixture_distribution}{{\tmem{mixture
distribution}}}.

\subsubsection{As a Generalization}

This model can also be interpreted as a direct generalization of
\href{https://arxiv.org/pdf/1601.00670.pdf}{mean-field variational inference}.
Indeed, let $N_{c} =1$, this model reduces to mean-field variational
inference. Remark that mean-field variational inference is a mature algorithm
and has been successfully established on many practical applications.

\subsubsection{As a Neural Network}

\subsection{Marginalization}

This model can be marginalized easily. This then benefits the transfering of
the model components. Precisely, for any dimension-index $\beta$ given, we can
marginalize all other dimensions directly, leaving
\begin{eqnarray*}
  q ( \theta_{\beta} ;a, \mu , \zeta ) & = & \prod_{\forall \gamma \neq \beta}
  \int \mathd \theta_{\gamma}   \sum_{i=1}^{N_{c}} c_{i} ( a ) \left\{
  \prod_{\alpha =1}^{d} \Phi ( \theta_{\alpha} ; \mu_{i  \alpha} , \sigma (
  \zeta_{i  \alpha} ) ) \right\}\\
  & = & \sum_{i=1}^{N_{c}} c_{i} ( a )   \Phi ( \theta_{\beta} ; \mu_{i 
  \beta} , \sigma ( \zeta_{i  \beta} ) ) ,
\end{eqnarray*}
where employed the normalization of $\Phi$.

\subsection{Loss-Function}

We use
\href{http://www.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf}{``evidence
of lower bound'' (ELBO)} as loss. It is ensured to have a unique global
minimal, at which $p ( \theta ;D ) =q ( \theta ;a, \mu , \zeta )$.
\begin{eqnarray*}
  \tmop{ELBO} ( a, \mu , \zeta ) & \assign & \mathbbm{E}_{\theta \sim q (
  \theta ;a, \mu , \zeta )} [ \ln  p ( \theta ;D ) - \ln  q ( \theta ;a, \mu ,
  \zeta ) ]\\
  & \approx & \left( \frac{1}{n}   \sum_{\theta_{( s )}} \right) \{ \ln  p (
  \theta_{( s )} ;D ) - \ln  q ( \theta_{( s )} ;a, \mu , \zeta ) \} ,
\end{eqnarray*}
where $\{ \theta_{( s )} : s=1, \ldots ,n \}$ is sampled from $q ( \theta ;a,
\mu , \zeta )$ as a distribution. Since there's no compact support for both $p
( \theta ;D )$ and $q ( \theta ;a, \mu , \zeta )$, $\tmop{ELBO}$ is
well-defined, as the loss-function (or say loss-function, performance, etc) of
the fitting.

\section{Stochastic Optimization}

\subsection{Difference between Bayesian and Traditional Methods}

Suppose, instead of use the whole dataset, we employ mini-batch technique.
Since all data are independent, if suppose that $D_{m}$ is unbiased in $D$,
then we have,
\[ \ln  p ( D| \theta ) = \sum_{D} p ( ( x_{i} ,y_{i} , \sigma_{i} ) | \theta
   ) \approx \frac{N_{D}}{N_{m}} \sum_{D_{m}} p ( ( x_{i} ,y_{i} , \sigma_{i}
   ) | \theta ) = \frac{N_{D}}{N_{m}}   \ln  p ( D_{m} | \theta ) . \]
Then,
\[ \ln  p ( \theta ;D ) = \ln  p ( D| \theta ) + \ln  p ( \theta ) =
   \frac{N_{D}}{N_{m}}   \ln  p ( D_{m} | \theta ) + \ln  p ( \theta ) , \]
thus as previous
\[ \ln  p ( \theta ;D ) = \frac{N_{D}}{N_{m}} \sum_{( x_{i} ,y_{i} ,
   \sigma_{i} ) \in D_{m}} \left\{ - \frac{1}{2} \ln   ( 2  \pi  
   \sigma_{i}^{2} ) - \frac{1}{2}   \left( \frac{y_{i} -f ( x_{i} ; \theta
   )}{\sigma_{i}} \right)^{2} \right\} + \ln  p ( \theta ) . \]
In this we meet one of the main differences between the Bayesian and the
traditional. In the traditional method, $N_{D}$ does not matters in training,
being absent in the optimizer. However, in Bayesian, the number of data that
are employed is encoded into Bayesian model, and has to, since the greater
number of data gives more confidence. So, while using stochastic optimization
in Bayesian mode, the factor $N_{D} /N_{m}$ of likelihood has to be taken into
account. We have to know how many data we actrually have, thus how confident
we are.

\section{ADVI}

Automatic differentation variational inference (ADVI)\footnote{See,
\href{https://arxiv.org/abs/1603.00788}{Kucukelbir, et al, 2016}.} has the
advantage that the variance of its Monte Carlo integral is orderly smaller
than that of black box variational inference (i.e. optimization directly using
ELBO without further reparameterization).

Precisely, let $\mathbbm{E}$ for mean value, $\mathbbm{H}$ for shannon
entropy, $\Phi$ for Gaussian, and $\sigma ( . )$ for softplus function. By
\[ q ( \theta ;a, \mu , \zeta ) = \sum_{i=1}^{N_{c}} w_{i} ( a )   \Phi (
   \theta ; \mu_{i} , \sigma ( \zeta_{i} ) ) , \]
we have
\begin{eqnarray*}
  \tmop{ELBO} & = & \mathbbm{E}_{q ( \theta ;a, \mu , \zeta )} [ \ln  p (
  \theta ;D ) ] +\mathbbm{H} [ q ( \theta ;a, \mu , \zeta ) ]\\
  & = & \sum_{i}^{N_{c}} w_{i} ( a )  \mathbbm{E}_{\Phi_{i} ( \theta ;
  \mu_{i} , \sigma ( \zeta_{i} ) )} [ \ln  p ( \theta ;D ) ] +\mathbbm{H} [ q
  ( \theta ;a, \mu , \zeta ) ]\\
  & =: & E_{1} +E_{2} .
\end{eqnarray*}
($E_{2}$ is analytic and independent of $p ( \theta ;D )$, so we leave it for
later.) Then, for $\forall i=1, \ldots ,N_{c}$, $\forall \alpha =1, \ldots
,N_{d}$, let
\[ \eta_{\alpha} \assign \frac{\theta_{\alpha} - \mu_{i \alpha}}{\sigma (
   \zeta_{i \alpha} )} , \]
we have $\theta_{\alpha} = \sigma ( \zeta_{i \alpha} )   \eta_{\alpha} +
\mu_{i \alpha}$ ($\theta = \sigma ( \zeta_{i} )   \eta + \mu_{i}$ if hides the
$\alpha$ index). So, for any $i$-components in the $E_{1}$ of $\tmop{ELBO}$,
we transform
\[ \mathbbm{E}_{\Phi ( \theta ; \mu_{i} , \sigma ( \zeta_{i} ) )} [ \ln  p (
   \theta ;D ) ] =\mathbbm{E}_{\Phi ( \eta ;0,1 )} [ \ln  p ( \sigma (
   \zeta_{i} )   \eta + \mu_{i} ;D ) ] . \]
Thus, we have derivatives
\begin{eqnarray*}
  \frac{\partial E_{1}}{\partial \mu_{i \alpha}} & = & w_{i} ( a ) 
  \mathbbm{E}_{\Phi ( \eta ;0,1 )} [ \nabla_{\alpha} \ln  p ( \sigma (
  \zeta_{i} )   \eta + \mu_{i} ;D ) ] ;\\
  \frac{\partial E_{1}}{\partial \zeta_{i \alpha}} & = & w_{i} ( a ) 
  \mathbbm{E}_{\Phi ( \eta ;0,1 )} \left[ \nabla_{\alpha} \ln  p ( \sigma (
  \zeta_{i} )   \eta + \mu_{i} ;D )   \eta_{\alpha}   \frac{\partial
  \sigma}{\partial \zeta_{i \alpha}} ( \zeta_{i} ) \right] ,
\end{eqnarray*}
where $\nabla_{\alpha} \ln  p \assign \partial \ln  p/ \partial
\theta_{\alpha}$ as the formal derivative. So, for these two, value of $\ln  p
( \theta ;D )$ is regardless.

\section{Deep Learning}

It cannot solve the vanishing gradient problem of deep neural network, since
this problem is intrinsic to the posterior of deep neural network. Indeed, the
posterior has the shape like $\exp ( -x^{2} / \sigma^{2} )$ with $\sigma
\rightarrow 0$, where $x$ is the variable (argument) of the posterior. It has
a sharp peak, located at a tiny area, with all other region extremely flat.
The problem of find this peak, or equivalently, findng its tiny area, is
intrinsically intactable.

So, even for Bayesian neural network, a layer by layer abstraction along depth
cannot be absent.

\section{Transfer Learning}

\section{Why not MCMC?}

\end{document}
